{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# 00_generate_synthetic_data.py\n",
        "import numpy as np, pandas as pd, os\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# ----- Parameters (35 years; ~1.1M rows -> fine locally) -----\n",
        "start = pd.Timestamp(\"1990-01-01\")\n",
        "end   = pd.Timestamp(\"2024-12-31\")\n",
        "categories = [\"Footwear\",\"Apparel\",\"Accessories\"]\n",
        "channels   = [\"Online\",\"Store\",\"Outlet\"]\n",
        "base_price = {\"Footwear\":120.0,\"Apparel\":60.0,\"Accessories\":30.0}\n",
        "base_demand= {\"Footwear\":200,   \"Apparel\":300,  \"Accessories\":400}\n",
        "\n",
        "# ----- Calendar & global signals -----\n",
        "cal = pd.date_range(start, end, freq=\"D\")\n",
        "cdf = pd.DataFrame({\"date\":cal})\n",
        "n = len(cdf); t = np.arange(n)\n",
        "cdf[\"dow\"]    = cdf[\"date\"].dt.dayofweek\n",
        "cdf[\"month\"]  = cdf[\"date\"].dt.month\n",
        "cdf[\"year\"]   = cdf[\"date\"].dt.year\n",
        "cdf[\"market_index\"] = 1.0 + 0.15*np.sin(2*np.pi*t/365.25) + 0.05*np.random.randn(n)\n",
        "rw = np.cumsum(np.random.normal(0,0.02,n))\n",
        "cdf[\"liquidity\"] = (rw - rw.min())/(rw.max()-rw.min())  # 0..1\n",
        "cdf[\"turnover\"]  = 1.0 + 0.10*np.cos(2*np.pi*t/90) + 0.05*np.random.randn(n)\n",
        "\n",
        "# ----- Expand over category × channel and simulate -----\n",
        "rows=[]\n",
        "for cat in categories:\n",
        "    for ch in channels:\n",
        "        df=cdf.copy()\n",
        "        # Base price with simple channel deltas\n",
        "        bp = base_price[cat]*(1 + (0.05 if ch==\"Online\" else 0.0) - (0.05 if ch==\"Outlet\" else 0.0))\n",
        "        df[\"base_price\"]=bp\n",
        "\n",
        "        # Liquidity → discount bands (17–24%)\n",
        "        low  = df[\"liquidity\"]<0.4\n",
        "        med  = (df[\"liquidity\"]>=0.4)&(df[\"liquidity\"]<0.7)\n",
        "        high = df[\"liquidity\"]>=0.7\n",
        "        disc = np.zeros(len(df))\n",
        "        disc[low]  = np.random.uniform(0.17,0.19, low.sum())\n",
        "        disc[med]  = np.random.uniform(0.19,0.22, med.sum())\n",
        "        disc[high] = np.random.uniform(0.22,0.24, high.sum())\n",
        "        disc += np.random.normal(0,0.003,len(df))\n",
        "        disc = np.clip(disc,0,0.50)\n",
        "        df[\"discount_rate\"]=disc\n",
        "        df[\"price\"] = df[\"base_price\"]*(1-df[\"discount_rate\"])\n",
        "\n",
        "        # Demand: diminishing returns to discount, + seasonality + market + turnover\n",
        "        a,b = 1.0, 2.2  # peak ≈ 20–22%\n",
        "        disc_mult  = 1 + a*df[\"discount_rate\"] - b*(df[\"discount_rate\"]**2)\n",
        "        weekend    = 1 + 0.08*(df[\"dow\"]>=5)  # Sat/Sun\n",
        "        peak_month = 1 + 0.10*(df[\"month\"].isin([8,9,11,12]))\n",
        "        exog_mult  = df[\"market_index\"]*(1 + 0.05*(df[\"turnover\"]-df[\"turnover\"].mean()))\n",
        "        base_d     = base_demand[cat]*(1.0 + (0.10 if ch==\"Online\" else -0.05 if ch==\"Outlet\" else 0.0))\n",
        "        lam = base_d * disc_mult * weekend * peak_month * exog_mult\n",
        "\n",
        "        qty = np.maximum(0, np.round(np.random.lognormal(np.log(lam/100.0), 0.25)*100.0)).astype(int)\n",
        "\n",
        "        df[\"category\"]=cat; df[\"channel\"]=ch\n",
        "        df[\"quantity_sold\"]=qty\n",
        "        df[\"revenue\"]=df[\"price\"]*df[\"quantity_sold\"]\n",
        "        rows.append(df)\n",
        "\n",
        "full = pd.concat(rows, ignore_index=True)\n",
        "\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "full.to_csv(\"data/adidas_daily.csv\", index=False)\n",
        "print(\"Saved data/adidas_daily.csv with\", len(full), \"rows\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XogWdPAIdhWp",
        "outputId": "42e21dbf-f636-4493-8214-7e12012ec7bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved data/adidas_daily.csv with 115056 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 10_ingest_clean_to_curated_local.py\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "spark = SparkSession.builder.appName(\"AdidasIngestCleanLocal\").getOrCreate()\n",
        "\n",
        "# Read generated CSV\n",
        "df = (spark.read.option(\"header\",True).option(\"inferSchema\",True)\n",
        "      .csv(\"data/adidas_daily.csv\"))\n",
        "\n",
        "# Cast & simple filters\n",
        "clean = (df\n",
        "  .withColumn(\"date\", col(\"date\").cast(\"date\"))\n",
        "  .withColumn(\"quantity_sold\", col(\"quantity_sold\").cast(\"int\"))\n",
        "  .withColumn(\"revenue\", col(\"revenue\").cast(\"double\"))\n",
        "  .withColumn(\"discount_rate\", col(\"discount_rate\").cast(\"double\"))\n",
        "  .withColumn(\"base_price\", col(\"base_price\").cast(\"double\"))\n",
        "  .withColumn(\"price\", col(\"price\").cast(\"double\"))\n",
        "  .withColumn(\"year\", col(\"year\").cast(\"int\"))\n",
        "  .filter(col(\"discount_rate\").between(0,0.9))\n",
        "  .filter(col(\"price\") > 0)\n",
        ")\n",
        "\n",
        "# Write LOCAL Parquet (curated)\n",
        "(clean\n",
        " .repartition(\"year\",\"category\")\n",
        " .write.mode(\"overwrite\")\n",
        " .partitionBy(\"year\",\"category\")\n",
        " .parquet(\"data/curated/\"))\n",
        "\n",
        "print(\"Wrote curated Parquet to data/curated/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaxpFPCleX16",
        "outputId": "7a3b3673-fa81-45f7-caf4-4cae722ab104"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote curated Parquet to data/curated/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 20_feature_engineering_to_features_local.py\n",
        "# Creates ML features (lags, moving averages, calendar) and writes LOCAL Parquet to data/features/\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import lag, avg, month, dayofweek, col\n",
        "\n",
        "spark = SparkSession.builder.appName(\"AdidasFeaturesLocal\").getOrCreate()\n",
        "\n",
        "base = spark.read.parquet(\"data/curated/\")\n",
        "\n",
        "w = Window.partitionBy(\"category\").orderBy(\"date\")\n",
        "\n",
        "feat = (base\n",
        "  .withColumn(\"qty_lag_1\",  lag(\"quantity_sold\", 1).over(w))\n",
        "  .withColumn(\"qty_lag_7\",  lag(\"quantity_sold\", 7).over(w))\n",
        "  .withColumn(\"qty_lag_30\", lag(\"quantity_sold\",30).over(w))\n",
        "  .withColumn(\"qty_ma_7\",   avg(\"quantity_sold\").over(w.rowsBetween(-6,0)))\n",
        "  .withColumn(\"qty_ma_30\",  avg(\"quantity_sold\").over(w.rowsBetween(-29,0)))\n",
        "  .withColumn(\"month\",      month(col(\"date\")))\n",
        "  .withColumn(\"dow\",        dayofweek(col(\"date\")))    # 1..7 (Sun..Sat)\n",
        "  .na.drop(subset=[\"qty_lag_1\",\"qty_lag_7\",\"qty_lag_30\",\"qty_ma_7\",\"qty_ma_30\"])\n",
        ")\n",
        "\n",
        "(feat\n",
        " .repartition(\"year\",\"category\")\n",
        " .write.mode(\"overwrite\")\n",
        " .partitionBy(\"year\",\"category\")\n",
        " .parquet(\"data/features/\"))\n",
        "\n",
        "print(\"Wrote features to data/features/\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVAGs8hvebSh",
        "outputId": "e1aa8bfe-34af-491a-a8da-3c27e7aace9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote features to data/features/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 30_train_models_local.py\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression, RandomForestRegressor\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "spark = SparkSession.builder.appName(\"AdidasTrainLocal\").getOrCreate()\n",
        "data = spark.read.parquet(\"data/features/\").cache()\n",
        "\n",
        "ev_rmse = RegressionEvaluator(metricName=\"rmse\", labelCol=\"label\", predictionCol=\"prediction\")\n",
        "ev_r2   = RegressionEvaluator(metricName=\"r2\",   labelCol=\"label\", predictionCol=\"prediction\")\n",
        "\n",
        "# -------- A) Discount level (HOW MUCH?) --------\n",
        "cols_level = [\"discount_rate\",\"liquidity\",\"turnover\",\"month\",\"dow\",\"base_price\"]\n",
        "vecA = (VectorAssembler(inputCols=cols_level, outputCol=\"features\")\n",
        "        .transform(data).select(\"features\", col(\"quantity_sold\").alias(\"label\")))\n",
        "trainA, testA = vecA.randomSplit([0.8,0.2], seed=42)\n",
        "\n",
        "en = LinearRegression(regParam=5e-4, elasticNetParam=0.2)\n",
        "rf = RandomForestRegressor(numTrees=150, maxDepth=12, seed=42)\n",
        "\n",
        "enm = en.fit(trainA); pred_en = enm.transform(testA)\n",
        "rfm = rf.fit(trainA); pred_rf = rfm.transform(testA)\n",
        "\n",
        "print(\"[LEVEL] ElasticNet  RMSE:\", ev_rmse.evaluate(pred_en), \" R2:\", ev_r2.evaluate(pred_en))\n",
        "print(\"[LEVEL] RandomForest RMSE:\", ev_rmse.evaluate(pred_rf), \" R2:\", ev_r2.evaluate(pred_rf))\n",
        "\n",
        "# Save models locally\n",
        "enm.write().overwrite().save(\"models/level_elasticnet\")\n",
        "rfm.write().overwrite().save(\"models/level_rf\")\n",
        "\n",
        "# -------- B) Timing (WHEN?) --------\n",
        "cols_time = [\"qty_lag_1\",\"qty_lag_7\",\"qty_lag_30\",\"qty_ma_7\",\"qty_ma_30\",\"month\",\"dow\"]\n",
        "vecB = (VectorAssembler(inputCols=cols_time, outputCol=\"features\")\n",
        "        .transform(data).select(\"features\", col(\"quantity_sold\").alias(\"label\")))\n",
        "trainB, testB = vecB.randomSplit([0.8,0.2], seed=42)\n",
        "\n",
        "rf_t = RandomForestRegressor(numTrees=200, maxDepth=14, seed=42)\n",
        "mod_t = rf_t.fit(trainB)\n",
        "pred_t = mod_t.transform(testB)\n",
        "\n",
        "print(\"[TIMING] RF RMSE:\", ev_rmse.evaluate(pred_t), \" R2:\", ev_r2.evaluate(pred_t))\n",
        "\n",
        "mod_t.write().overwrite().save(\"models/timing_rf\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YKITLGbsehYy",
        "outputId": "bc27cec9-3e80-4ee7-edbf-570b38d67173"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LEVEL] ElasticNet  RMSE: 111.65269021767224  R2: 0.4417837810153328\n",
            "[LEVEL] RandomForest RMSE: 102.12478610309496  R2: 0.5329897772007931\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o3374.fit.\n: org.apache.spark.SparkException: Job 54 cancelled because SparkContext was shut down\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:1301)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:1299)\n\tat scala.collection.mutable.HashSet$Node.foreach(HashSet.scala:450)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:376)\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:1299)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:3234)\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:85)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$stop$3(DAGScheduler.scala:3120)\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1300)\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:3120)\n\tat org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:2346)\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1300)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2346)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2297)\n\tat org.apache.spark.SparkContext.$anonfun$new$36(SparkContext.scala:704)\n\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:231)\n\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:205)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:205)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:205)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:184)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2549)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1057)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1056)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:740)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:739)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:665)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.runBagged(RandomForest.scala:210)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:304)\n\tat org.apache.spark.ml.regression.RandomForestRegressor.$anonfun$train$1(RandomForestRegressor.scala:159)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:226)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:226)\n\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:137)\n\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:46)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:79)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2631061521.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mrf_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumTrees\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxDepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mmod_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0mpred_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             raise TypeError(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/ml/util.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFuncT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mtry_remote_fit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mtry_remote_fit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1362\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    328\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o3374.fit.\n: org.apache.spark.SparkException: Job 54 cancelled because SparkContext was shut down\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:1301)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:1299)\n\tat scala.collection.mutable.HashSet$Node.foreach(HashSet.scala:450)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:376)\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:1299)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:3234)\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:85)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$stop$3(DAGScheduler.scala:3120)\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1300)\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:3120)\n\tat org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:2346)\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1300)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2346)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2297)\n\tat org.apache.spark.SparkContext.$anonfun$new$36(SparkContext.scala:704)\n\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:231)\n\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:205)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:205)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:205)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:184)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2549)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1057)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1056)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:740)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:739)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:665)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.runBagged(RandomForest.scala:210)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:304)\n\tat org.apache.spark.ml.regression.RandomForestRegressor.$anonfun$train$1(RandomForestRegressor.scala:159)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:226)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:226)\n\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:137)\n\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:46)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:79)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LgayMAC9gqpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 40_visualize_dashboards.py\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import sum as ssum\n",
        "\n",
        "spark = SparkSession.builder.appName(\"AdidasDashboards\").getOrCreate()\n",
        "\n",
        "# ---- A) Liquidity-based discount policy (visual guide) ----\n",
        "fig, ax = plt.subplots(figsize=(9,4.8))\n",
        "ax.add_patch(patches.Rectangle((0.15,17),0.25,2, color='#8da0cb', alpha=0.45, label='Low liquidity: 17–19%'))\n",
        "ax.add_patch(patches.Rectangle((0.40,19),0.30,3, color='#66c2a5', alpha=0.45, label='Medium liquidity: 19–22%'))\n",
        "ax.add_patch(patches.Rectangle((0.70,22),0.30,2, color='#fc8d62', alpha=0.45, label='High liquidity: 22–24%'))\n",
        "ax.axhline(17, color='gray', ls='--'); ax.axhline(24, color='gray', ls='--')\n",
        "ax.text(0.82, 24.2, 'Overall optimal range', color='gray')\n",
        "ax.set_xlim(0,1); ax.set_ylim(16,25)\n",
        "ax.set_xlabel('Liquidity (0 = low, 1 = high)'); ax.set_ylabel('Recommended Discount (%)')\n",
        "ax.legend(loc='upper left'); ax.grid(alpha=0.2)\n",
        "plt.tight_layout(); plt.savefig(\"outputs/liquidity_discount_strategy.png\", dpi=160); plt.close()\n",
        "\n",
        "# ---- B) Actual (last 10) vs Forecast (next 20) ----\n",
        "daily = (spark.read.parquet(\"hdfs:///projects/adidas/curated/daily/\")\n",
        "         .groupBy(\"date\").agg(ssum(\"quantity_sold\").alias(\"qty\"))\n",
        "         .orderBy(\"date\"))\n",
        "pdf = daily.toPandas()\n",
        "\n",
        "# Take last 30 actuals → linear trend → forecast next 20\n",
        "recent = pdf.tail(30).reset_index(drop=True)\n",
        "import numpy as np\n",
        "x = np.arange(len(recent)); coef = np.polyfit(x, recent[\"qty\"].values, 1)\n",
        "trend = np.poly1d(coef); horizon = 20\n",
        "future_x = np.arange(len(recent), len(recent)+horizon)\n",
        "forecast_vals = trend(future_x)*(1+np.random.normal(0,0.02,horizon))\n",
        "forecast_vals = np.maximum(0, forecast_vals)\n",
        "\n",
        "plt.figure(figsize=(9,4.8))\n",
        "plt.plot(np.arange(1,11), recent[\"qty\"].values[-10:], label=\"Actual Demand\", color=\"#1f77b4\")\n",
        "plt.plot(np.arange(11,31), forecast_vals, label=\"Forecasted Demand\", color=\"#ff7f0e\")\n",
        "plt.xticks(range(1,31,2)); plt.xlabel(\"Day\"); plt.ylabel(\"Demand (units)\")\n",
        "plt.legend(); plt.grid(alpha=0.2); plt.tight_layout()\n",
        "plt.savefig(\"outputs/forecast_dashboard.png\", dpi=160); plt.close()\n",
        "\n",
        "print(\"Saved dashboards to outputs/*.png\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "L1V6UVj9ZWSV",
        "outputId": "782da48e-37c2-4d06-f412-35e9d7269a7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ConnectionRefusedError",
          "evalue": "[Errno 111] Connection refused",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2935782631.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msum\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mssum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AdidasDashboards\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# ---- A) Liquidity-based discount policy (visual guide) ----\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m                     \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m                     \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_j_spark_session_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m                     \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplyModifiableSettings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_get_j_spark_session_module\u001b[0;34m(jvm)\u001b[0m\n\u001b[1;32m    670\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_j_spark_session_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjvm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"JVMView\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"JavaObject\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjvm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"org.apache.spark.sql.classic.SparkSession$\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MODULE$\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_repr_html_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1750\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mUserHelpAutoCompletion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1752\u001b[0;31m         answer = self._gateway_client.send_command(\n\u001b[0m\u001b[1;32m   1753\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREFLECTION_COMMAND_NAME\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1754\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1034\u001b[0m          \u001b[0;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \"\"\"\n\u001b[0;32m-> 1036\u001b[0;31m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconnection\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_new_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36m_create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_parameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             self.gateway_property, self)\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect_to_java_server\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_thread_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36mconnect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    436\u001b[0m                 self.socket = self.ssl_context.wrap_socket(\n\u001b[1;32m    437\u001b[0m                     self.socket, server_hostname=self.java_address)\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_address\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_port\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_connected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
          ]
        }
      ]
    }
  ]
}